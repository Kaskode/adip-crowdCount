{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"colab":{"name":"main.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"code","metadata":{"id":"7Dain3YbLVTl"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5rSIqYE3LCS7","executionInfo":{"status":"ok","timestamp":1610401490094,"user_tz":-60,"elapsed":5778,"user":{"displayName":"ERNEST OFOSU ADDO","photoUrl":"","userId":"14556292728805351526"}}},"source":["# libraries and dependencies\n","\n","import h5py\n","import scipy.io as io\n","import PIL.Image as Image\n","import numpy as np\n","import os\n","import glob\n","from matplotlib import pyplot as plt\n","from scipy.ndimage.filters import gaussian_filter \n","import scipy\n","import scipy.spatial\n","import json\n","from matplotlib import cm as CM\n","from image import *\n","import torch\n","import torch.nn as nn\n","import shutil\n","from torchvision import models\n","from tqdm import tqdm\n","from torchvision import datasets, transforms\n","import sys\n","sys.path.append('/content/drive/MyDrive/ADIP/')\n","from model import CSRNet\n","from utils import save_net, load_net\n","%matplotlib inline"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"bgnIxpXULCTE"},"source":["# gaussian filter function: generate density map with adaptive kernel\n","\n","def gaussian_filter_density(gt):\n","    print (gt.shape)\n","    density = np.zeros(gt.shape, dtype=np.float32)\n","    gt_count = np.count_nonzero(gt)\n","    if gt_count == 0:\n","        return density\n","\n","    pts = np.array(list(zip(np.nonzero(gt)[1], np.nonzero(gt)[0])))\n","\n","    leafsize = 2048\n","    # build kdtree\n","    tree ################################################## TRAINING MODEL ####################################################################= scipy.spatial.KDTree(pts.copy(), leafsize=leafsize)\n","    # query kdtree\n","    distances, locations = tree.query(pts, k=4)\n","\n","    print ('generate density...')\n","    for i, pt in enumerate(pts):\n","        pt2d = np.zeros(gt.shape, dtype=np.float32)\n","        pt2d[pt[1],pt[0]] = 1.\n","        if gt_count > 1:\n","            sigma = (distances[i][1]+distances[i][2]+distances[i][3])*0.1\n","        else:\n","            sigma = np.average(np.array(gt.shape))/2./2. #case: 1 point\n","        density += scipy.ndimage.filters.gaussian_filter(pt2d, sigma, mode='constant')\n","    print ('done.')\n","    return density"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"G9fZ9p-3LCTF"},"source":["# set the root to the dataset directory\n","\n","root = '/content/drive/MyDrive/ADIP/ShanghaiTech_Crowd_Counting_Dataset/'\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nAP1VfLwm9Pm"},"source":["# generate paths to image folders in dataset\n","\n","part_A_train = os.path.join(root,'part_A_final/train_data','images')\n","part_A_test = os.path.join(root,'part_A_final/test_data','images')\n","part_B_train = os.path.join(root,'part_B_final/train_data','images')\n","part_B_test = os.path.join(root,'part_B_final/test_data','images')\n","path_sets = [part_A_train,part_A_test]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XjayRzpSmFAN"},"source":["# paths for Part A images\n","img_paths = []\n","for path in path_sets:\n","    for img_path in glob.glob(os.path.join(path, '*.jpg')):\n","        img_paths.append(img_path)\n","for path in path_sets:\n","    for img_path in glob.glob(os.path.join(path, '*.jpg')):\n","        img_paths.append(img_path)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"iHRs3kNvLCTH"},"source":["# generate the density maps for images in Part A of dataset\n","\n","for img_path in img_paths:\n","    print (img_path)\n","    mat = io.loadmat(img_path.replace('.jpg','.mat').replace('images','ground_truth').replace('IMG_','GT_IMG_'))\n","    img= plt.imread(img_path)\n","    k = np.zeros((img.shape[0],img.shape[1]))\n","    gt = mat[\"image_info\"][0,0][0,0][0]\n","    for i in range(0,len(gt)):\n","        if int(gt[i][1])<img.shape[0] and int(gt[i][0])<img.shape[1]:\n","            k[int(gt[i][1]),int(gt[i][0])]=1\n","    k = gaussian_filter_density(k)\n","    with h5py.File(img_path.replace('.jpg','.h5').replace('images','ground_truth'), 'w') as hf:\n","            hf['density'] = k"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-QDIyMg3LCTH"},"source":["# visualize an image sample from Part A\n","\n","plt.imshow(Image.open(img_paths[0]))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"r3O4J803LCTI"},"source":["# density map of sample image\n","\n","gt_file = h5py.File(img_paths[0].replace('.jpg','.h5').replace('images','ground_truth'),'r')\n","groundtruth = np.asarray(gt_file['density'])\n","plt.imshow(groundtruth,cmap=CM.jet)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"2loBclxwLCTI"},"source":["# ground truth (count estimate) of sample image\n","\n","np.sum(groundtruth)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"5qiFmkzeLCTI"},"source":["# pathsets for Part B of dataset\n","\n","path_sets = [part_B_train,part_B_test]\n","\n","img_paths = []\n","for path in path_sets:\n","    for img_path in glob.glob(os.path.join(path, '*.jpg')):\n","        img_paths.append(img_path)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"4uwOMUjtLCTJ"},"source":["# generate the density maps for images in Part B of dataset\n","\n","for img_path in img_paths:\n","    print(img_path)\n","    mat = io.loadmat(img_path.replace('.jpg','.mat').replace('images','ground_truth').replace('IMG_','GT_IMG_'))\n","    img= plt.imread(img_path)\n","    k = np.zeros((img.shape[0],img.shape[1]))\n","    gt = mat[\"image_info\"][0,0][0,0][0]\n","    for i in range(0,len(gt)):\n","        if int(gt[i][1])<img.shape[0] and int(gt[i][0])<img.shape[1]:\n","            k[int(gt[i][1]),int(gt[i][0])]=1\n","    k = gaussian_filter(k,15)\n","    with h5py.File(img_path.replace('.jpg','.h5').replace('images','ground_truth'), 'w') as hf:\n","            hf['density'] = k"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RGqIoaVoqV7Q"},"source":["# visualize an image sample from Part B\n","\n","plt.imshow(Image.open(img_paths[0]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"P3p5TTJlqgtv"},"source":["# density map of sample image\n","\n","gt_file = h5py.File(img_paths[0].replace('.jpg','.h5').replace('images','ground_truth'),'r')\n","groundtruth = np.asarray(gt_file['density'])\n","plt.imshow(groundtruth,cmap=CM.jet)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HDPd9ukEqna_"},"source":["# ground truth (count estimate) of sample image\n","np.sum(groundtruth)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"T2tW0zndwZH2"},"source":["# train model and save weights\n","# Shanghai A\n","!python /content/drive/MyDrive/ADIP/train.py /content/drive/MyDrive/ADIP/part_A_train.json /content/drive/MyDrive/ADIP/part_A_val.json 0 0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"h1VRWcHjXf9H"},"source":["# train model and save weights\n","# Shanghai B\n","!python /content/drive/MyDrive/ADIP/train.py /content/drive/MyDrive/ADIP/part_B_train.json /content/drive/MyDrive/ADIP/part_B_val.json 0 0"],"execution_count":null,"outputs":[]}]}